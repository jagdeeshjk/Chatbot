{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac988444",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "\n",
    "!pip install --upgrade keras\n",
    "\n",
    "!pip install --upgrade matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3015b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved as train_data.pickle\n",
      "Test data saved as test_data.pickle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Directory where the extracted bAbI files are located\n",
    "input_directory = r\"C:\\Onedrive IITK\\OneDrive - IIT Kanpur\\Project Data science\\My chatbot\\tasks_1-20_v1-2\\en-10k\"  # Update this path accordingly\n",
    "\n",
    "# Specify the file names\n",
    "train_file_name = \"qa6_yes-no-questions_train.txt\"\n",
    "test_file_name = \"qa6_yes-no-questions_test.txt\"\n",
    "\n",
    "# Construct full file paths\n",
    "train_file_path = os.path.join(input_directory, train_file_name)\n",
    "test_file_path = os.path.join(input_directory, test_file_name)\n",
    "\n",
    "# Read train and test files\n",
    "with open(train_file_path, 'r', encoding='utf-8') as train_file:\n",
    "    train_data = train_file.read()\n",
    "\n",
    "with open(test_file_path, 'r', encoding='utf-8') as test_file:\n",
    "    test_data = test_file.read()\n",
    "\n",
    "# Save as pickle files\n",
    "train_pickle_file = \"train_data.pickle\"\n",
    "test_pickle_file = \"test_data.pickle\"\n",
    "\n",
    "with open(train_pickle_file, 'wb') as train_pickle:\n",
    "    pickle.dump(train_data, train_pickle)\n",
    "\n",
    "with open(test_pickle_file, 'wb') as test_pickle:\n",
    "    pickle.dump(test_data, test_pickle)\n",
    "\n",
    "print(f\"Train data saved as {train_pickle_file}\")\n",
    "print(f\"Test data saved as {test_pickle_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d81674",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pickle.load(train_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029dc741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data.pickle\",\"rb\") as fp:\n",
    "    train_data=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc24bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.pickle\",\"rb\") as fp:\n",
    "    test_data=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b29fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84127e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e679a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a75aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_to_list(data_string):\n",
    "    data_list = []\n",
    "    lines = data_string.strip().split('\\n')\n",
    "    for i in range(2, len(lines)):\n",
    "        current_line = lines[i]\n",
    "        parts = current_line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            #question = parts[0].strip().split()\n",
    "            question = [word for word in parts[0].strip().split() if not word.isdigit()]\n",
    "            answer = parts[1].strip()\n",
    "              # Extract previous two lines and split them word-wise into lists\n",
    "            prev1_words = [word for word in lines[i-1].strip().split() if not word.isdigit()]\n",
    "            prev2_words = [word for word in lines[i-2].strip().split() if not word.isdigit()]\n",
    "            \n",
    "            # Join the first two sentences into a single string\n",
    "            prev1_and_prev2 = ' '.join(prev2_words + prev1_words)\n",
    "            \n",
    "            # Append a tuple of combined previous two lines, question, and answer to data_list\n",
    "            data_list.append((prev1_and_prev2.split(), question, answer))\n",
    "    return data_list\n",
    "# Parse string data into list of tuples\n",
    "test_data = parse_string_to_list(test_data)\n",
    "\n",
    "# Print the parsed data\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb03727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30017955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_to_list(data_string):\n",
    "    data_list = []\n",
    "    lines = data_string.strip().split('\\n')\n",
    "    for i in range(2, len(lines)):\n",
    "        current_line = lines[i]\n",
    "        parts = current_line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            #question = parts[0].strip().split()\n",
    "            question = [word for word in parts[0].strip().split() if not word.isdigit()]\n",
    "            answer = parts[1].strip()\n",
    "              # Extract previous two lines and split them word-wise into lists\n",
    "            prev1_words = [word for word in lines[i-1].strip().split() if not word.isdigit()]\n",
    "            prev2_words = [word for word in lines[i-2].strip().split() if not word.isdigit()]\n",
    "            \n",
    "            # Join the first two sentences into a single string\n",
    "            prev1_and_prev2 = ' '.join(prev2_words + prev1_words)\n",
    "            \n",
    "            # Append a tuple of combined previous two lines, question, and answer to data_list\n",
    "            data_list.append((prev1_and_prev2.split(), question, answer))\n",
    "    return data_list\n",
    "\n",
    "# Parse string data into list of tuples\n",
    "train_data = parse_string_to_list(train_data)\n",
    "\n",
    "# Print the parsed data\n",
    "print(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93187841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca498869",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd727c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = test_data+train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8badbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question, answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('yes')\n",
    "vocab.add('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aecd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f154dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len=len(vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in all_data:\n",
    "    print (len(data[0]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_len = max([len(data[0]) for data in all_data])\n",
    "max_ques_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaeb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c229006",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ace4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story, question, answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    " len(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d20460",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_story_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f484f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_index = tokenizer.word_index, max_story_len = max_story_len, max_ques_len = max_ques_len):\n",
    "    X=[]#stories\n",
    "    Xq=[] #query/question\n",
    "    Y=[] #correct answer\n",
    "    for story, query, answer in data:\n",
    "        x=[word_index[word.lower()] for word in story]\n",
    "        xq=[word_index[word.lower()] for word in query]\n",
    "        y= np.zeros(len(word_index)+1)\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return(pad_sequences(X, maxlen = max_story_len),pad_sequences(X, maxlen = max_ques_len),np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train,queries_train,answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9288d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test,queries_test,answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6acd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea87890",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0363c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input((max_story_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = Input((max_ques_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510898e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim = vocab_len, output_dim = 64))\n",
    "input_encoder_m.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4872b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim = vocab_len, output_dim = max_ques_len))\n",
    "input_encoder_c.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim = vocab_len, output_dim = 64, input_length = max_ques_len))\n",
    "question_encoder.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = dot([input_encoded_m, question_encoded], axes=(2,2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1db76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = add([match,input_encoded_c])\n",
    "response = Permute((2,1))(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = concatenate([response,question_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dff1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10ffa5df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951177bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = LSTM(32)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_len)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = Activation('softmax')(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eee080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_sequence, question], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1954e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb995f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([inputs_train, queries_train], answers_train, batch_size = 32, epochs = 20, validation_data = ([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend([\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"chatbot_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dda754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"chatbot_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([inputs_test, queries_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = ' '.join(word for word in test_data[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adee33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(word for word in test_data[10][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c26e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede007b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[10][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_max=np.argmax(pred_results[10])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k=key\n",
    "        \n",
    "print(\"Predicted Answer is\", k)\n",
    "print(\"Probability of certainty\",pred_results[10][val_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ff56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make our own story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Mary dropped the football. Sandra discarded the apple in kitchen. Daniel went to office.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfe587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "story.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d784c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = \"Is Daniel in the office.\"\n",
    "my_question.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [(story.split(), my_question.split(),'yes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story, my_ques, my_ans = vectorize_stories(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([my_story,my_ques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_max=np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k=key\n",
    "        \n",
    "print(\"Predicted Answer is\", k)\n",
    "print(\"Probability of certainty\",pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c3c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7cd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
